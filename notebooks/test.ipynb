{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b730ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully imported Arabert and Grover GPT-2 model\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from arabert import ArabertPreprocessor \n",
    "from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\n",
    "print(\"Sucessfully imported Arabert and Grover GPT-2 model\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb8319",
   "metadata": {},
   "source": [
    "# Fine-tuning Pipeline \n",
    "## Data Loading \n",
    "  \n",
    "## Preprocessing \n",
    "## Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0808a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded 20402 tweets\n",
      "                                                text  label\n",
      "0  والله ما كاين لخرج علي تلاميذ من غيركم لجينه ن...      0\n",
      "1  لا توجد مستعمراات اسبانيه ولكن توجد المساله ال...      0\n",
      "2  akbare nasabe nahaba lmalayire layatawara3o fi...      1\n",
      "3  h 9dima dik l3ba yama hergthom bach tchdo assu...      0\n",
      "4  نحن درنا التلقيح ورغم دلك اصبنا ملي درنا التلق...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "# train test split dataset from sklearn \n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "model_name = \"aubmindlab/aragpt2-base\"  \n",
    "dataset_path = \"../datasets/Moroccan_Darija_Offensive_Language_Detection_Dataset.csv\"\n",
    "\n",
    "# Load dataset \n",
    "df = pd.read_csv(dataset_path) \n",
    "print(f\"Dataset Loaded {len(df)} tweets\")\n",
    "print(df.head()) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53250cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init arabert preprocessor \n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "train_tweets, val_tweets, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(), \n",
    "    df['label'].tolist(),\n",
    "    test_size=0.25, \n",
    "    random_state=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c77df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2deb92eec354c8b9ebd095610c48181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/added_tokens.json. We won't load it.\n",
      "Didn't find file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/tokenizer_config.json. We won't load it.\n",
      "loading file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/vocab.json\n",
      "loading file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"/Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 5,\n",
      "      \"repetition_penalty\": 3.0,\n",
      "      \"top_p\": 0.95\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 15301\n",
      "Validation dataset size: 5101\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch \n",
    "from huggingface_hub import snapshot_download \n",
    "\n",
    "\n",
    "# Tokenization \n",
    "model_path = snapshot_download(repo_id=model_name) \n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path) \n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a pad token, so we use eos token\n",
    "\n",
    "def tokenize_function(tweets, labels) : \n",
    "    encodings = tokenizer(tweets, truncation=True, padding=True, max_length=128) \n",
    "    data = [] \n",
    "    for i in range(len(tweets)) : \n",
    "        row = {key: torch.tensor(value[i]) for key, value in encodings.items()}\n",
    "        row['labels'] = torch.tensor(labels[i])\n",
    "        data.append(row) \n",
    "    return data\n",
    "\n",
    "train_dataset = tokenize_function(train_tweets, train_labels)\n",
    "val_dataset = tokenize_function(val_tweets, val_labels)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3823ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 5,\n",
      "      \"repetition_penalty\": 3.0,\n",
      "      \"top_p\": 0.95\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
      "\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /Users/saidibenariba/.cache/huggingface/hub/models--aubmindlab--aragpt2-base/snapshots/257d7289dac3605a6cd903ff71d67a52fc5d09aa and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer are set up and ready for training.\n"
     ]
    }
   ],
   "source": [
    "## model loading \n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id  # Set pad token ID to eos token ID\n",
    "\n",
    "print(\"Model and tokenizer are set up and ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd32e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/arabert_final/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15301\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fine tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f36d964e7b458993cf8b6c1485eb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7729, 'learning_rate': 4.9912876807806244e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5901, 'learning_rate': 4.982575361561248e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6777, 'learning_rate': 4.973863042341871e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6083, 'learning_rate': 4.9651507231224954e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7049, 'learning_rate': 4.956438403903119e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6828, 'learning_rate': 4.947726084683743e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6062, 'learning_rate': 4.939013765464367e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6701, 'learning_rate': 4.9303014462449906e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6275, 'learning_rate': 4.921589127025614e-05, 'epoch': 0.05}\n",
      "{'loss': 0.7139, 'learning_rate': 4.912876807806238e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5956, 'learning_rate': 4.9041644885868624e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6442, 'learning_rate': 4.895452169367486e-05, 'epoch': 0.06}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "def compute_metrics(pred) : \n",
    "    labels = pred.label_ids \n",
    "    preds = pred.predictions.argmax(-1) \n",
    "    acc = accuracy_score(labels, preds) \n",
    "    f1 = f1_score(labels, preds, average='weighted') \n",
    "    return {\n",
    "        'accuracy' : acc, \n",
    "        'f1' : f1\n",
    "    }\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results_aragpt2\", \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8, \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    logging_dir=\"./logs\", \n",
    "    logging_steps=10, \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"f1\",\n",
    ") \n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_arguments, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Start Fine tuning...\") \n",
    "trainer.train() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arabert_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
